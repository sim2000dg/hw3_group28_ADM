{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/simonedigregorio/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/simonedigregorio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/simonedigregorio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/simonedigregorio/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/simonedigregorio/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import feature_extraction\n",
    "import dotenv\n",
    "import sys\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "dotenv.load_dotenv('../ext_variables.env') # Necessary to avoid putting absolute paths\n",
    "os.chdir(os.getenv(\"PATH_FILES_ADM\"))\n",
    "tqdm.pandas() # add tqdm progress_apply method for Pandas classes (DataFrame, Series and GroupBy classes)\n",
    "nltk.download('wordnet') # wordnet\n",
    "nltk.download(\"stopwords\") # stopword list\n",
    "nltk.download('punkt') # sentence separation\n",
    "nltk.download('averaged_perceptron_tagger') # pos-tagging\n",
    "nltk.download('omw-1.4') # wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# First of all, let's load the csv/the tsv\n",
    "places  = pd.read_csv(\"places.csv\", sep = \"\\t\", index_col=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Search Engine\n",
    "<a id = \"point_2\"></a>\n",
    "\n",
    "First of all, let's perform some pre-processing with lemmatization (incorporating POS tagging), stopwords removal (what remains of them at that point) and lowercase conversion on the _description_ columns.\n",
    "\n",
    "The first function in the next block of code converts the tag from the [Penn Treebank project](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) to the ones of [WordNet](https://wordnet.princeton.edu).\n",
    "\n",
    "The second function performs the whole-preprocessing itself:\n",
    " - First of all, it tokenizes the strings with `word_tokenize`. This may seem simple, but it is not. Under the hood, `nltk` uses an already trained (for English) unsupervised model able to understand how to split the string into sentence (one can retrain the unsupervised model with a particular corpus, with a specific target language). The output is then parsed with a RegEx expression in order to be split into words.\n",
    "  - The tokenized output is then passed to the part-of-speech tagger, whose details can be found in a very good blog post, [here](https://explosion.ai/blog/part-of-speech-pos-tagger-in-python).\n",
    "  - The tags are mapped to the ones from WordNet with the function `get_wordnet_pos`, the first defined in the block. Notice that tags not related to adjectives, verbs, nouns or adverbs will be cancelled out by the function, returning `None`. Once this is done, if the resulting tag has `None` value, the related token/word is not considered.\n",
    "  - With the tuples `(token, pos_tag)` we can finally call the WordNet lemmatizer (more specifically, the _morphy_ function, more info [here](https://wordnet.princeton.edu/documentation/morphy7wn)).\n",
    "  - The output is converted to lower case and checked against stopwords (and against some punctuation which may still be there).\n",
    "  - The words are joined together with the `|` separator, a character that I do not expect to be popular in the English language.\n",
    "\n",
    "Notice that with this process we have implicitly removed punctuation."
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For the following mapping, credits to this stack overflow question: https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# The following is a function which performs the whole pre-processing with lemmatization (incorporating information from POS tagging), stopwords removal and lowercase conversion\n",
    "def preprocessing_field(description: str) -> str:\n",
    "    # First of all, we need to tokenize the text.\n",
    "    # We use word_tokenize from nltk\n",
    "    tokenized = word_tokenize(description, language='english')\n",
    "    # Then we use the pos tagger from nltk\n",
    "    pos_tagged = nltk.pos_tag(tokenized)\n",
    "    # Then we convert tags from Penn Treebank format to the one of WordNet\n",
    "    # We also perform some manual filtering of punctuation that may still be there\n",
    "    converted_tags_words = [(x[0], get_wordnet_pos(x[1])) for x in pos_tagged]\n",
    "    # Filter everything that is not an adverb, a verb, a noun or an adjective\n",
    "    converted_tags_words = [(x[0], x[1]) for x in converted_tags_words if x[1] is not None]\n",
    "    # Lemmatize the words (WordNet lemmatizer, morphy function)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word = x[0], pos = x[1]) for x in converted_tags_words]\n",
    "    # Lowercase conversion and stopwords removal (most of the stopwords were anyway removed with get_wordnet_pos and related filtering)\n",
    "    # Some residual punctuation is also removed. Notice that stopwords removal could be done also later on with CountVectorizer and scikit, but we do it here\n",
    "    lemmatized_words = [x.lower() for x in lemmatized_words if (x.lower() not in stopwords.words(\"english\")) and (x not in (\"’\", \"‘\", \"\\'\", \"‟\", \"”\", \"-\",\"“\",\".\"))]\n",
    "    # Join the words into strings with words separated explicitly (afterwards we will only need to specify the separator for scikit learn CountVectorizer)\n",
    "    return \"|\"+\"|\".join(lemmatized_words)+\"|\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following is just a little example to show the power of what we are doing right now. It is a little homage to Breiman, from his Wikipedia page.\n",
    "\n",
    "Also notice that, contrary to what we do with stemming, lemmatization retains something which can be understood and read; this in general improves visualization, reporting and debugging with text data. Additionally, lemmatization preserves far more variability than stemming, for obvious reasons (we are reducing words to their dictionary lemma, not to their root), and this may be useful since we are working on a search engine, whose queries may aim at specific words.\n",
    "\n",
    "However, we may not always want this variability, and this is indeed the case for some specific applications, such as ML modelling where we prefer to catch the signal while disregarding noise due to specific lemmas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preprocessing_field(\"Leo Breiman was a distinguished statistician at the University of California, Berkeley. He was the recipient of numerous honors and awards, and was a member of the United States National Academy of Sciences. Breiman's work helped to bridge the gap between statistics and computer science, particularly in the field of machine learning. His most important contributions were his work on classification and regression trees and ensembles of trees fit to bootstrap samples. Bootstrap aggregation was given the name bagging by Breiman. Another of Breiman's ensemble approaches is the random forest.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we apply what we have described to the whole _descriptions_ columns. We are using the integration between `pandas` and `tqdm` in order to keep track of progress."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lemmatization of Description and Short Description with POS tagging\n",
    "# We simply assign the description columns to the transformed descriptions\n",
    "places.loc[:, \"placeDesc\"], places.loc[:, \"placeShortDesc\"] = places.placeDesc.progress_apply(preprocessing_field), places.placeShortDesc.progress_apply(preprocessing_field)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "places.placeDesc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Conjunctive query\n",
    "<a id = \"point_2.1\"></a>\n",
    "\n",
    "### 2.1.1 Create your index\n",
    "\n",
    "Next step is building the inverted index. In order to do that, the first step is building a so-called Term-Document matrix, which is easily built with scikit-learn, more specifically with its (very, very useful) `CountVectorizer` class. The `transform` method of the class returns a Document-Term matrix, so we need to take its transpose. For every row (a document) we will have a list (along the columns) of one-hot representations (presence or absence of a word).\n",
    "\n",
    "We need to pass a RegEx expression to enforce the separator we have placed before: `|`. We also pass other arguments to the `__init__` method, but they are relatively straightforward.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The RegEx token pattern is very simple here, it just has a single capturing group with a non-greedy quantifier and a lookahead and a lookbehind to avoid consuming \"|\" in the match, which is a NO-GO\n",
    "# Binary = TRUE => one hot encoding representation\n",
    "# We avoid considering tokens appearing only once\n",
    "one_hot_vectorizer = feature_extraction.text.CountVectorizer(strip_accents=False, lowercase=False, token_pattern=r\"(?<=\\|)(.*?)(?=\\|)\", min_df=2, binary = True)\n",
    "one_hot_vectorizer.fit(places.placeDesc) # Get the vocabulary from the corpus\n",
    "document_term = one_hot_vectorizer.transform(places.placeDesc).transpose() # Transform the corpus into the document-term matrix, then take the transpose of the output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then get the dictionary mapping each word to its index in the matrix. In order to do that, we can just access an attribute of the one_hot_vectorizer object. It will be saved, serialized, in storage. For that we can just use the `pickle` Python module."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "# The vocabulary is saved, serialized, in storage\n",
    "vocabulary_word_index = one_hot_vectorizer.vocabulary_\n",
    "with open(\"vocabulary_word_index.pickle\", \"wb\") as vocab_file:\n",
    "    pickle.dump(vocabulary_word_index, file = vocab_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to get the dictionary/the inverted index we just need to extract the indexes of the non-zero entries in the sparse Document-Term matrix. Since the `sparse.csc_matrix` class of _scipy_ (whose instance is returned by the `transform` method of the `CountVectorizer` class) has a method for this, we can just use that one. What is returned, as usual, are two NumPy arrays, one for the indexes referring to the rows, and one for the indexes referring to the columns. They are two iterables, so we `zip` them together and iterate over them in order to build the inverted index."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "inverted_index_onehot = defaultdict(list)\n",
    "for row in zip(document_term.nonzero()[0], document_term.nonzero()[1]):\n",
    "    inverted_index_onehot[row[0]].append(row[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
