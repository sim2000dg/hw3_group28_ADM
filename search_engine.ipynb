{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import feature_extraction\n",
    "import numpy as np\n",
    "import heapq\n",
    "import dotenv\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "dotenv.load_dotenv('../ext_variables.env') # Necessary to avoid putting absolute paths\n",
    "os.chdir(os.getenv(\"PATH_FILES_ADM\"))\n",
    "tqdm.pandas() # add tqdm progress_apply method for Pandas classes (DataFrame, Series and GroupBy classes)\n",
    "nltk.download('wordnet') # wordnet\n",
    "nltk.download(\"stopwords\") # stopword list\n",
    "nltk.download('punkt') # sentence separation\n",
    "nltk.download('averaged_perceptron_tagger') # pos-tagging\n",
    "nltk.download('omw-1.4') # wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# First of all, let's load the csv/the tsv\n",
    "places  = pd.read_csv(\"places.tsv\", sep = \"\\t\", index_col=0)\n",
    "places.drop_duplicates(inplace=True) # Removing duplicates coming from the scraping and crawling process"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Search Engine\n",
    "<a id = \"point_2\"></a>\n",
    "\n",
    "First of all, let's perform some pre-processing with lemmatization (incorporating POS tagging), stopwords removal (what remains of them at that point) and lowercase conversion on the _description_ columns.\n",
    "\n",
    "The first function in the next block of code converts the [tags from the Penn Treebank project](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) to the ones of [WordNet](https://wordnet.princeton.edu), a powerful word database with a peculiar hierarchical structure. We need the Wordnet tags because the lemmatizer we are using requires that kind of tag.\n",
    "\n",
    "The second function performs the whole-preprocessing itself:\n",
    " - First of all, it tokenizes the strings with `word_tokenize`. This may seem simple, but it is not. Under the hood, `nltk` uses an already trained (for English) unsupervised model able to understand how to split the string into sentences (one can retrain the unsupervised model with a particular corpus, with a specific target language). The output is then parsed with a RegEx expression in order to be split into words.\n",
    "  - The tokenized output is then passed to the NLTK part-of-speech tagger.\n",
    "  - The tags are mapped to the ones from WordNet with the function `get_wordnet_pos`, the first defined in the block. Notice that tags not related to adjectives, verbs, nouns or adverbs will be cancelled out by the function, which in that case returns `None`. Once this is done, if the resulting tag has `None` value, the related token/word is not considered.\n",
    "  - With the tuples `(token, pos_tag)` we can finally call the lemmatizer (more specifically, the _morphy_ function, more info [here](https://wordnet.princeton.edu/documentation/morphy7wn)), and the output is converted to lower case (the output of morphy is lower case, but unsuccessful lemmatization leads to having the unchanged input token returned).\n",
    "  - If the lemmatization returns a lemma which is not present in any of the Wordnet synsets (groups of words with related meaning) we discard the token.\n",
    "  - The output is checked against stopwords.\n",
    "  - The words are joined together with the `|` separator, a character that I do not expect to be popular in the English language.\n",
    "\n",
    "Notice that with this process we have implicitly removed punctuation."
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For the following mapping, credits to this stack overflow question:\n",
    "# https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('RB'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# The following is a function which performs the whole pre-processing with lemmatization\n",
    "# (incorporating information from POS tagging), stopwords removal and lowercase conversion\n",
    "def preprocessing_field(description: str) -> str:\n",
    "    description = re.sub(r\"(?<=\\S)\\.(?=\\S)\", \". \", description) # Add a space after a point where needed in order to enforce proper separation of sentences\n",
    "    # First of all, we need to tokenize the text.\n",
    "    # We use word_tokenize from nltk\n",
    "    tokenized = word_tokenize(description, language='english')\n",
    "    # Then we use the pos tagger from nltk\n",
    "    pos_tagged = nltk.pos_tag(tokenized)\n",
    "    # Then we convert tags from Penn Treebank format to the one of WordNet\n",
    "    converted_tags_words = [(x[0], get_wordnet_pos(x[1])) for x in pos_tagged]\n",
    "    # Filter everything that is not an adverb, a verb, a noun or an adjective\n",
    "    converted_tags_words = [(x[0], x[1]) for x in converted_tags_words if x[1] is not None]\n",
    "    # Lemmatize the words (morphy function) and convert to lower case\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word = x[0], pos = x[1]).lower() for x in converted_tags_words]\n",
    "    # Remove returned lemmas which are not in any of the synsets in WordNet\n",
    "    lemmatized_words = [x for x in lemmatized_words if wordnet.synsets(x)]\n",
    "    # Stopwords removal (most of the stopwords were anyway removed with get_wordnet_pos and related filtering)\n",
    "    # Notice that stopwords removal could also be done later on with CountVectorizer and scikit, but we do it here\n",
    "    lemmatized_words = [x for x in lemmatized_words if (x not in stopwords.words(\"english\"))]\n",
    "    # Join the words into strings with words separated explicitly\n",
    "    # (afterwards we will only need to specify the separator for scikit learn CountVectorizer)\n",
    "    return \"|\"+\"|\".join(lemmatized_words)+\"|\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following is just an example to show what we are doing right now. The text comes from the Wikipedia page of Leo Breiman.\n",
    "\n",
    "Also notice that, contrary to what we do with stemming, lemmatization retains something which can be understood and read; this in general improves visualization, reporting and debugging with text data. Additionally, lemmatization preserves far more variability than stemming, for obvious reasons (we are reducing words to their dictionary lemma, not to their root), and this may be useful since we are working on a search engine, whose queries may aim at specific words.\n",
    "\n",
    "However, we may not always want this variability, and this is indeed the case for some specific applications, such as ML modelling where we prefer to catch the signal while disregarding low-level details due to specific lemmas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preprocessing_field(\"Leo Breiman was a distinguished statistician at the University of California, Berkeley. He was the recipient of numerous honors and awards, and was a member of the United States National Academy of Sciences.Breiman's work helped to bridge the gap between statistics and computer science, particularly in the field of machine learning. His most important contributions were his work on classification and regression trees and ensembles of trees fit to bootstrap samples. Bootstrap aggregation was given the name bagging by Breiman. Another of Breiman's ensemble approaches is the random forest.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we apply what we have described to the whole _descriptions_ columns. We are using the integration between `pandas` and `tqdm` in order to keep track of progress. Also notice that we are not overwriting the previous columns (the search engine should return a polished output, not the columns pre-processed for the search engine)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lemmatization of Description and Short Description with POS tagging\n",
    "# We simply assign the description columns to the transformed descriptions\n",
    "places[\"placeDesc_post\"], places[\"placeShortDesc_post\"] = places.placeDesc.progress_apply(preprocessing_field), places.placeShortDesc.progress_apply(preprocessing_field)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Conjunctive query\n",
    "<a id = \"point_2.1\"></a>\n",
    "\n",
    "Next step is building the inverted index. In order to do that, the first step is building a so-called Term-Document matrix, which is easily built with scikit-learn, more specifically with its (very, very useful) `CountVectorizer` class. The `transform` method of the class returns a Document-Term matrix, so we need to take its transpose. For every row (a document) we will have a list (along the columns) of one-hot representations (presence or absence of a word).\n",
    "\n",
    "We need to pass a RegEx expression to enforce the separator we have placed before: `|`. We also pass other arguments to the `__init__` method, but they are relatively straightforward.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The RegEx token pattern is very simple here, it just has a single capturing group with a non-greedy quantifier and a lookahead and a lookbehind to avoid consuming \"|\" in the match, which is a NO-GO\n",
    "# Binary = TRUE => one hot encoding representation\n",
    "one_hot_vectorizer = feature_extraction.text.CountVectorizer(strip_accents=False, lowercase=False, token_pattern=r\"(?<=\\|)(.*?)(?=\\|)\", binary = True)\n",
    "one_hot_vectorizer.fit(places.placeDesc_post) # Get the vocabulary from the corpus\n",
    "term_document = one_hot_vectorizer.transform(places.placeDesc_post).transpose() # Transform the corpus into the document-term matrix, then take the transpose of the output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then get the dictionary mapping each word to its index in the matrix. In order to do that, we can just access an attribute of the one_hot_vectorizer object. It will be saved, serialized, in storage. For that we can just use the `pickle` Python module."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "# The vocabulary is saved, serialized, in storage\n",
    "vocabulary_word_index = one_hot_vectorizer.vocabulary_\n",
    "with open(\"vocabulary_word_index.pickle\", \"wb\") as vocab_file:\n",
    "    pickle.dump(vocabulary_word_index, file = vocab_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to get the dictionary/the inverted index we just need to extract the indexes of the non-zero entries in the sparse Document-Term matrix. Since the `sparse.csc_matrix` class of _scipy_ (whose instance is returned by the `transform` method of the `CountVectorizer` class) has a method for this, we can just use that one. What is returned, as usual, are two NumPy arrays, one for the indexes referring to the rows, and one for the indexes referring to the columns. They are two iterables, so we `zip` them together and iterate over them in order to build the inverted index.\n",
    "\n",
    "Again, the resulting dictionary is saved in storage as a serialized object with the `pickle` module."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "inverted_index_onehot = defaultdict(list)\n",
    "for row in zip(term_document.nonzero()[0], term_document.nonzero()[1]):  # Build iterator that returns row and column index of non zero at each iteration\n",
    "    inverted_index_onehot[row[0]].append(row[1])\n",
    "with open(\"inverted_index_onehot.pickle\", \"wb\") as inverted_index_file:\n",
    "    pickle.dump(inverted_index_onehot, file = inverted_index_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to load them into memory from storage we simply need to use `pickle` again, this time with the `load` function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"vocabulary_word_index.pickle\", \"rb\") as vocab_file:\n",
    "    vocabulary_word_index = pickle.load(vocab_file)\n",
    "with open(\"inverted_index_onehot.pickle\", \"rb\") as inverted_index_file:\n",
    "    inverted_index_onehot = pickle.load(inverted_index_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "At this point, we just need to define the search engine function. It is enough to take the intersection between the sets/the lists (the values in the dictionary) containing the document ids for each of the terms in order to get the output of the search engine from the original DataFrame."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "def search_engine1(vocabulary:dict[str:int], inverted_index:dict[int: list[int, ...]], dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function asks for a query and retrieves the rows from the input DataFrame whose pre-processed placeDesc entry contains all the words in the query.\n",
    "    It does this by completely relying on an inverted index data structure, which in turn relies on a vocabulary dictionary in order to be mapped back to the\n",
    "    original string representations of the words/tokens.\n",
    "    :param vocabulary: The vocabulary dictionary mapping the string representation for each token to the related integer index\n",
    "    :param inverted_index: The inverted index mapping each index to a list of documents ids\n",
    "    :param dataframe: The Pandas DataFrame from which to retrieve the places. Notice that the function assumes that the Dataframe has [placeName, placeURL and placeDesc] in its column index\n",
    "    :return: a Pandas DataFrame with all the documents containing all the words in the query\n",
    "    \"\"\"\n",
    "    query = input(\"Query: \").strip() # Ask for input (removing leading and trailing whitespace)\n",
    "    query_elements = re.split(r'\\s+', query.lower()) # Split according to one or more whitespace with a simple expression\n",
    "    query_elements = list(set(query_elements)) # Get only unique words\n",
    "    token_ids = list(map(vocabulary.get, query_elements)) # Get the ids for each of the tokens from the vocabulary. None is returned if requested token is not present. Note that we have to get a list from the map object; this is due to the fact that the returned map object is an iterator, and thus the __iter__ method returns the object itself.\n",
    "    output_docs = []\n",
    "    for id in token_ids:\n",
    "        # If a term is missing in the vocabulary, the loop is immediately stopped, and the DataFrame equivalent of None is returned by passing an empty list to Pandas indexing.\n",
    "        if not all(token_ids):\n",
    "            break\n",
    "        if not output_docs: # Output docs is empty list at first, and it evaluates to False\n",
    "            output_docs = set(inverted_index[id]) # Initialize the set of the output docs (1st term)\n",
    "        else:\n",
    "            output_docs = output_docs.intersection(set(inverted_index[id])) # Take the intersection for each term after the first\n",
    "    output = dataframe.iloc[list(output_docs)][[\"placeName\", \"placeDesc\", \"placeURL\"]]\n",
    "    output.placeURL = output.placeURL.str.replace(r\"https://www.atlasobscura.com\", \"\", regex = False) # Remove redundant part of the URL\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Result for \\'American Museum\\':')\n",
    "search_engine1(vocabulary_word_index, inverted_index_onehot, places)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conjunctive query & ranking score\n",
    "<a id = \"point_2.2\"></a>\n",
    "\n",
    "The first step here is similar if not identical to the one for [Point 2.1](#point_2.1). We use again scikit-learn, but this time with the TF-IDF Vectorizer. We already have the dictionary, so we pass it to the constructor method of the class. Notice that the computed sparse document term matrix has its row vectors already L2-normalized. This makes our lives easier when computing the cosine similarity later on, as we will explain.\n",
    "\n",
    "The idf definition used by scikit when `smooth_idf=False` (we do not need the smoothing here, since the vocabulary is built directly from the corpus) is the following (can be checked by looking at the [documentation page](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer) of the `TfidfTransformer` class):\n",
    "$$\n",
    "idf(t) = \\log\\left(\\frac{N}{df(t)}\\right) + 1\n",
    "$$\n",
    "\n",
    "This means that the baseline for the tf-idf is 1 and not 0."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The RegEx token pattern is very simple here, it just has a single capturing group with a non-greedy quantifier and a lookahead and a lookbehind to avoid consuming \"|\" in the match, which is a NO-GO\n",
    "# Binary = TRUE => one hot encoding representation\n",
    "tfidf_vectorizer = feature_extraction.text.TfidfVectorizer(strip_accents=False, lowercase=False, token_pattern=r\"(?<=\\|)(.*?)(?=\\|)\", vocabulary = vocabulary_word_index, smooth_idf=False)\n",
    "tfidf_vectorizer.fit(places.placeDesc_post)\n",
    "term_document_tfidf = tfidf_vectorizer.transform(places.placeDesc_post).transpose() # Transform the corpus into the document-term matrix (with tf-idf weights), then take the transpose of the output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have to build the new inverted index with the tf-idf information. The way we build it is very similar to what we have done in [Point 2.1](#Point_2.1). The main difference here is that we need to incorporate the tf-idf information, with the value for each key in the dictionary becoming a list of tuples.\n",
    "\n",
    "First of all, we need to access the non-zero elements of the sparse matrix returned by the `transform` method of the `TfidfVectorizer` class. We can just use some fancy indexing on the sparse matrix object to do that."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# %%prun # Profile the code to check that fancy indexing is working (documentation for SciPy is really not up to the standards)\n",
    "# I wanted to be sure that it was NOT iterating over the indexes in order to retrieve the elements from the sparse matrix. In other words, I wanted to check the vectorization of the retrieval op\n",
    "from collections import defaultdict\n",
    "inverted_index_tfidf = defaultdict(list)\n",
    "\n",
    "nonzero_entries = term_document_tfidf.nonzero() # Get indexes (for rows and for columns) of the non-zero entries in the sparse matrix\n",
    "\n",
    "flattened_sparse = np.asarray(term_document_tfidf[nonzero_entries[0], nonzero_entries[1]]).flatten() # Flatten the matrix to a vector of its non-zero entries (simple fancy indexing op)\n",
    "for row in zip(nonzero_entries[0], nonzero_entries[1], flattened_sparse):  # Build iterator that returns row and column index + value of non-zero entries at each iteration\n",
    "    inverted_index_tfidf[row[0]].append((row[1], row[2])) # Appending tuples to the list assigned to each term/token/key\n",
    "with open(\"inverted_index_tfidf.pickle\", \"wb\") as inverted_index_file:\n",
    "    pickle.dump(inverted_index_tfidf, file = inverted_index_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this second search engine, we need to build upon the first one by considering the cosine similarity between the query itself and the tf-idf representation of the documents. Since we do expect users to only write short queries with unique words, it makes sense to consider the _angle_ given by the one-hot encoding of the unique words in the query, without highlighting them considering their frequency in the corpus (through the idf score).\n",
    "\n",
    "The reasoning is the following: if a user writes some words for the query, they think that those words are equally relevant for the search, so it does not make sense to give them a differentiated weight in the query representation; in the documents, those frequent words are relatively penalized w.r.t. infrequent ones, and it makes sense that they are, since the editors of a document write according to a syntactic and semantic flow, and common words may be common just because they are common in a language and in the context of the corpus.\n",
    "\n",
    "Therefore, in order for the document vectors to be more or less parallel to the query representation, they need to have **balanced** tf-idf entries for all the words present in the query. A query with both a common and a rare word returns a high cosine similarity if the document vector has more occurrences of the common word than it has for the rarer word; this is due to the fact that the common word is very meaningful in the query and not meaningful at all in the document, so the term frequency at document level needs to compensate that.\n",
    "\n",
    "For the following implementation, please also notice another thing: as we said above, the tf-idf representations are already **L2 normalized**, and this means that the cosine similarity is easier to compute. With $\\textbf{q}$ as the one-hot encoded representation of the query and $\\textbf{d}$ as the normalized tf-idf representation of the document, the cosine similarity is this way defined (due to the L2 normalization):\n",
    "\n",
    "$$\n",
    "\\theta = \\frac{\\textbf{q}\\cdot\\textbf{d}}{||\\textbf{q}||_2||\\textbf{d}||_2} = \\frac{\\textbf{q}\\cdot\\textbf{d}}{||\\textbf{q}||_2}, ||\\textbf{d}||_2 = 1\n",
    "$$\n",
    "\n",
    "In our implementation, this can be simplified to dividing the sum of the entries in the tf-idf document vectors that are related to the unique words present in the query by the square root of the number of unique words in the query. It is trivial to show this, given that the query representation is a one-hot encoded vector."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def search_engine2(vocabulary:dict[str:int], inverted_index:dict[int: list[int, ...]], dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function asks for a query and retrieves the 5 top rows/places from the input DataFrame whose pre-processed placeDesc entry contains all the words in the query.\n",
    "    The 5 top places are extracted by using a max-heap and they are defined according to the cosine similarity of the placeDesc entry/document with the query.\n",
    "    The pipeline completely relies on an inverted index data structure, which in turn relies on a vocabulary dictionary in order to be mapped back to the original string\n",
    "    representations of the words/tokens.\n",
    "    :param vocabulary: The vocabulary dictionary mapping the string representation for each token to the related integer index\n",
    "    :param inverted_index: The inverted index mapping each index to a list of tuples, each containing a documents id and a related tf-idf value\n",
    "    :param dataframe: The Pandas DataFrame from which to retrieve the places. Notice that the function assumes that the Dataframe has [placeName, placeURL and placeDesc] in its column index\n",
    "    :return: a Pandas DataFrame with the top 5 documents according to cosine similarity with the query\n",
    "    \"\"\"\n",
    "    query = input(\"Query: \").strip() # Ask for input (removing leading and trailing whitespace)\n",
    "    query_elements = re.split(r'\\s+', query.lower()) # Split according to one or more whitespace with a simple expression\n",
    "    query_elements = list(set(query_elements)) # Get only unique words\n",
    "    output_docs = None\n",
    "    token_ids = list(map(vocabulary.get, query_elements)) # Map each of the tokens to their ids/values in the dictionary. None is returned if not present. None is falsy\n",
    "\n",
    "\n",
    "    for id in token_ids:\n",
    "        # If a term is missing in the vocabulary, the function is immediately stopped, and the DataFrame equivalent of None is returned by passing an empty list to Pandas indexing\n",
    "        if not all(token_ids):\n",
    "            return dataframe.iloc[[]][[\"placeName\", \"placeDesc\", \"placeURL\"]]\n",
    "        if not output_docs: # Output docs is None at first, and it evaluates to False\n",
    "            output_docs = set([x[0] for x in inverted_index[id]]) # Initialize the set of the output docs (1st term)\n",
    "        else:\n",
    "            output_docs = output_docs.intersection(set([x[0] for x in inverted_index_tfidf[id]])) # Take the intersection for each term after the first\n",
    "\n",
    "\n",
    "    # Now we need to sort according to TF-IDF and cosine similarity with the query\n",
    "    output_docs = list(output_docs) # We need an ordered structure, set has no order\n",
    "    output_array = np.empty(shape=(len(token_ids), len(output_docs)), dtype=np.float64) # Allocate array which will contain the tfidf values for each of the query token for each of the output documents\n",
    "\n",
    "    for iter_index, id in enumerate(token_ids):\n",
    "        tf_idf_dict = dict(inverted_index_tfidf[id]) # When we pass a list of tuples (such as in this case) to the dictionary constructor, they are considered key:values pairs\n",
    "        output_array[iter_index] = list(map(tf_idf_dict.get, output_docs)) # each row contains the tf_idf values (one for each output doc) for a specific token\n",
    "\n",
    "\n",
    "    # We are doing a lot in the following row. First of all we are computing the cosine similarity between each of the docs and the query (refer to the description above this function\n",
    "    # definition to understand the reasoning). The second thing we do is creating an iterator of tuples (cosine similarity and document ids) with zip. Then we exhaust that iterator and move\n",
    "    # its elements into a list\n",
    "    heap_output = list(zip(-output_array.sum(0)/np.sqrt(output_array.shape[0]), output_docs))\n",
    "    # Change the order of the elements of the list (in place) in order to make it represent a heap. The result is a max heap because we have changed the sign of the entries heapify returns an array for a min heap\n",
    "    heapq.heapify(heap_output)\n",
    "\n",
    "\n",
    "    index_df = [] # Initialize list containing indexes of the documents/DataFrame rows\n",
    "    cosine_sim = [] # Initialize list containing cosine similarity\n",
    "    for i in range(min(5, len(heap_output))):\n",
    "        doc = heapq.heappop(heap_output) # Repeated heappop after heapify is exactly equivalent to heapsort\n",
    "        index_df.append(doc[1])\n",
    "        cosine_sim.append(-doc[0])\n",
    "\n",
    "    output_df = dataframe.iloc[index_df][[\"placeName\", \"placeDesc\", \"placeURL\"]]\n",
    "    output_df[\"cosine_sim\"] = cosine_sim\n",
    "    output_df.placeURL = output_df.placeURL.str.replace(r\"https://www.atlasobscura.com\", \"\", regex = False) # Remove redundant part of the URL\n",
    "\n",
    "    return output_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Result for \\'American Museum\\':')\n",
    "search_engine2(vocabulary_word_index, inverted_index_tfidf, places)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Define a new score!\n",
    "<a id=\"point3\"></a>\n",
    "\n",
    "In this case we have to compute a new score in order to build a new search engine. More things need to be considered:\n",
    "- First thing that comes to mind is considering the name of the place itself.\n",
    "- Second thing in a ranking of importance is the country the user is aiming at.\n",
    "- Third thing, in order to distinguish between popular and niche places, we need to consider both the number of people that want to visit and the number of people that have visited the places.\n",
    "\n",
    "In order to build the new ranking system/the new score, we can ask the user to input additional information after the starting query, and then use that in order to do some additional manipulations and numerical operations. Let's analyze in depth what we are doing here:\n",
    "- The name of the place is factored in by asking the user for some words he may remember and/or search from the name of the place. For all the $N_t$ input words, if the name of the place contains one of the words: $n_{t, i} \\leftarrow n_{t, i}+1$, with $n_{t, i}\\leftarrow 0 $ for initialization. The final score for this point is given by $t_i = \\frac{n_{t, i}}{N_t} $. Notice that we obtain something bounded between 0 and 1.\n",
    "- The part of the score related to the country is way easier. $c_i = 1 \\iff place_i \\in CountryQueried$. To compute this we can just look at the last part of the `placeAddress` field of the original DataFrame.\n",
    "- Scoring according to popularity is a bit more sophisticated. We are asking for 1 to 3 as popularity, and this translates back to intervals stemming from the percentiles of the (complete) sample distribution of the `numPeopleWant` `numPeopleVisited` columns. 1 as popularity means querying for something in the 33 percentile, 2 as popularity means querying for something between the 33 percentile and the 66 percentile and 3 as popularity means residing above the 66 percentile. From checking this we obtain two booleans (one for `numPeopleWant` and one for `numPeopleVisited`), and by taking their mean (True is 1, False is 0) we obtain something bounded between 0 and 1. This score is $p_i$.\n",
    "\n",
    "The final value for the ith place is given, with $cs_i$ as the cosine similarity, by: $s_i = 0.1cs_i + 0.3t_i + 0.3c_i + 0.3p_i$. This can be seen as a weighted average between the scores, with the cosine similarity being given the lowest score due to the fact the information regarding the description entry is already used in order to build the list of places to score in the first place.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def search_engine3(vocabulary:dict[str:int], inverted_index:dict[int: list[int, ...]], dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function asks for a query and 3 additional pieces of information, regarding the name of the place(s), the country of the places and the popularity of the places.\n",
    "    It first retrieves the places/rows from the input DataFrame whose pre-processed placeDesc entry contains all the words in the query.\n",
    "    Each resulting place is scored according to the cosine similarity of the placeDesc(ription) entry with the starting query, the number of input words in the name of the place,\n",
    "    the match with the requested country and the match with the requested popularity. 5 top places are extracted by using a max-heap data structure (in array representation, a list)\n",
    "    on the score and by repeatedly popping the root of the heap (heapsort).\n",
    "    :param vocabulary: The vocabulary dictionary mapping the string representation for each token to the related integer index\n",
    "    :param inverted_index: The inverted index mapping each index to a list of tuples, each containing a documents id and a related tf-idf value\n",
    "    :param dataframe: The Pandas DataFrame from which to retrieve the places. Notice that the function assumes that the Dataframe has [placeName, placeURL and placeDesc] in its column index\n",
    "    :return: a Pandas DataFrame with the top 5 documents according to the new score\n",
    "    \"\"\"\n",
    "\n",
    "    query = input(\"Query: \").strip()\n",
    "\n",
    "    score_att = []\n",
    "    for i in range(3):\n",
    "        if i == 0:\n",
    "            score_att.append(input(\"Words from the name of the place: \").strip())\n",
    "        elif i == 1:\n",
    "            score_att.append(input('Countries, comma separated: ').strip())\n",
    "        elif i == 2:\n",
    "            score_att.append(input(\"How much popular, from 1 to 3: \").strip())\n",
    "\n",
    "    # Get documents where at least one of the words specified in the query is present\n",
    "\n",
    "    query_elements = re.split(r'\\s+', query.lower()) # Split according to one or more whitespace with a simple expression\n",
    "    query_elements = list(set(query_elements)) # Get only unique words\n",
    "    output_docs = None\n",
    "    token_ids = list(map(vocabulary.get, query_elements)) # Map each of the tokens to their ids/values in the dictionary. None is returned if not present. None is falsy\n",
    "\n",
    "\n",
    "    for id in token_ids:\n",
    "        # If a term is missing in the vocabulary, the function is immediately stopped, and the DataFrame equivalent of None is returned by passing an empty list to Pandas indexing\n",
    "        if not all(token_ids):\n",
    "            return dataframe.iloc[[]][[\"placeName\", \"placeDesc\", \"placeURL\"]]\n",
    "        if not output_docs: # Output docs is None at first, and it evaluates to False\n",
    "            output_docs = set([x[0] for x in inverted_index[id]]) # Initialize the set of the output docs (1st term)\n",
    "        else:\n",
    "            output_docs = output_docs.intersection(set([x[0] for x in inverted_index_tfidf[id]])) # Take the intersection for each term after the first\n",
    "\n",
    "    output_docs = list(output_docs) # We need an ordered structure, set has no order. These are the documents/the observations for which we have to compute the score\n",
    "\n",
    "    total_score = np.zeros(len(output_docs), dtype=np.float64) # Initialize array containing overall scores\n",
    "\n",
    "    # Cosine similarity part of the score computation\n",
    "    output_array = np.empty(shape=(len(token_ids), len(output_docs)), dtype=np.float64) # Allocate array which will contain the tfidf values for each of the query token for each of the output documents\n",
    "    for iter_index, id in enumerate(token_ids):\n",
    "        tf_idf_dict = dict(inverted_index_tfidf[id])\n",
    "        output_array[iter_index] = list(map(tf_idf_dict.get, output_docs)) # each row contains the tf_idf values (one for each output doc) for a specific token\n",
    "\n",
    "\n",
    "    # The content of this line is commented in depth in the definition of the search engine for point 2.2\n",
    "    total_score += 0.1 * np.squeeze(np.array(list(zip(output_array.sum(0)/np.sqrt(output_array.shape[0])))), 1) # update the total score (now a zero vector), element-wise\n",
    "\n",
    "    docs_df = dataframe.iloc[output_docs] # This is needed to access the other fields without continuously calling iloc with the doc ids\n",
    "\n",
    "    # Name of the place part of the score computation\n",
    "    if score_att[0]:\n",
    "        name_pieces = re.split(r'\\s+', score_att[0]) # Split the string containing name words\n",
    "        name_array = np.empty(shape=(len(name_pieces), len(docs_df)), dtype=np.bool_) # Initialize array of boolean (the documents have or not the specific word? len(name_pieces) words, len(docs_df) documents\n",
    "        for iter_index, piece in enumerate(name_pieces):\n",
    "            name_array[iter_index] = docs_df.placeName.str.contains(rf'\\s{piece}\\s', regex=True, flags=re.IGNORECASE) # assign a row to boolean 1D array coming from Pandas string op that checks condition\n",
    "        total_score += 0.3*name_array.mean(axis=0) # update the total score, element-wise with mean over boolean fields (1s and 0s)\n",
    "\n",
    "\n",
    "    # Country part of the score computation\n",
    "    if score_att[1]:\n",
    "        countries = '|'.join([x.strip() for x in score_att[1].split(',')]) # Build part of RegEx pattern to check\n",
    "        total_score += 0.3 * docs_df.placeAdress.str.match(rf'.*,\\s({countries})$', flags=re.IGNORECASE) # Complete and use RegEx pattern to check country\n",
    "\n",
    "    # Popularity part of the score computation\n",
    "    if score_att[2]:\n",
    "        percentile_interval = ((1/3)*(float(score_att[2])-1), (1/3)*float(score_att[2])) # Get the interval we are aiming at\n",
    "        # The score is simply the mean between the two boolean fields (1s and 0s) which are obtained by checking if the observation is in the required interval for the two features\n",
    "        pop_score = np.array((docs_df[[\"numPeopleVisited\", \"numPeopleWant\"]] > places[['numPeopleVisited', 'numPeopleWant']].quantile(percentile_interval[0], interpolation = 'higher')) & (docs_df[[\"numPeopleVisited\", \"numPeopleWant\"]] < places[['numPeopleVisited', 'numPeopleWant']].quantile(percentile_interval[1], interpolation = 'higher'))).mean(axis = 1)\n",
    "        total_score += 0.3 * pop_score # Update total score element-wise\n",
    "\n",
    "    # Build the heap from the scores + the ids\n",
    "    heap_output = list(zip(-total_score, output_docs)) # Build a list from zipped total scores and output docs\n",
    "    heapq.heapify(heap_output) # Heapify ---> build max heap\n",
    "\n",
    "    output_id = []\n",
    "    output_ordered_scores = []\n",
    "    for i in range(min(5, len(heap_output))): # Heap\n",
    "        popped = heapq.heappop(heap_output) # Heapsort\n",
    "        output_id.append(popped[1])\n",
    "        output_ordered_scores.append(-popped[0])\n",
    "\n",
    "    output_df = places.iloc[output_id][[\"placeName\", \"placeDesc\", \"placeURL\", \"placeAdress\", \"placeTags\", \"numPeopleVisited\", \"numPeopleWant\", \"placeAlt\", \"placeLong\"]]\n",
    "    output_df.insert(3, \"new_score\", output_ordered_scores)\n",
    "    output_df.placeURL = output_df.placeURL.str.replace(r\"https://www.atlasobscura.com\", \"\", regex = False) # Remove redundant part of the URL\n",
    "    return output_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "third_output = search_engine3(vocabulary_word_index, inverted_index_tfidf, places)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Output for American Museum + \\'medical\\' + \\'Thailand, United States, France\\' + 1 as popularity: \")\n",
    "third_output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that without any additional information (w.r.t. the initial query) the score is the exact same of what was done in [Point 2.2](#point2.2), just with a scaled cosine similarity. A user thus could easily revert to the previous search engine. That said, I think that this third engine works better since it allows the users to refine the queries and thus get specific places highlighted according to features he may remember and/or search for. With more information, auspiciously, we should get better results.\n",
    "\n",
    "As an example of this new scoring system, let's look at the _Siriraj Medical Museum case_, which appeared as the first result in the second search engine. In this case, it is still first, but just because the additional features we have passed to the engine actually match the place. Let us go through it together. _American Museum_ is the same query as always, so the cosine similarity is the same as before, $+0.354277\\cdot0.1$; _medical_ is contained in the title, so $+ 0.3$; _Thailand_, the country the place is in, is in the countries we have passed to the engine, so $+ 0.3$; and then $+0.15$ for the popularity since we have passed 1 and the place is above the 33 percentile for the `numPeopleVisited` field and below it for the `numPeopleWant` field. Summing everything up we obtain 0.7854277, which, approximated to the 6th decimal, is exactly 0.785428, the value we have in the table."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Visualizing the most relevant places\n",
    "\n",
    "In order to visualize the most relevant places we can use interactive maps with **Plotly**. A **bubble plot** may be useful in order to understand the number of people who have visited each place, while for everything else we should exploit the interactivity of the plot.\n",
    "\n",
    "As one can see in the code, we have indeed pushed the information concerning the address and the name of the place (required for the exercise) in the text boxes popping up when hovering over the point/the place on the map. The relative size of the marker increases as the number of people that have visited the place grows. That piece of information is thus conveyed visually.\n",
    "\n",
    "The country boundaries should be enough to understand which country the place is in, but for what concerns address and name there was no cleaner way than to use hover text. Also notice that the initial zoom for the map is automatically selected in order for it to be just enough to capture all the points in the window.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Set the overall plot, with red as color and triangle as shape for the markers. The size of the markers is given by the number of people that have visited the place\n",
    "# the fitbounds argument automatically zooms the map\n",
    "fig_places = px.scatter_geo(third_output, lat = 'placeAlt', lon = 'placeLong', color_discrete_sequence = ['red'], symbol_sequence = ['triangle-up-dot'], opacity = 0.9, width = 1300, height=700, size = 'numPeopleVisited', fitbounds='locations')\n",
    "\n",
    "# Add title and subtitle (we can use HTML in Plotly)\n",
    "fig_places.update_layout(title=go.layout.Title(text = 'Interactive geospatial visualization of the 5 most relevant places according to user query<br>'\n",
    "                                               '<sup>Size of the marker grows with the number of '\n",
    "                                               'people that have visited the place. Hovering over the points returns additional information.</sup>',\n",
    "                                               xref=\"paper\", font=dict(size=25, family='Droid Serif')))\n",
    "\n",
    "# Add country borders with black color, change color of the lands (I am a bit color-blind, so don't judge XD) and remove lakes from visualization\n",
    "fig_places.update_geos(showcountries=True, countrycolor = 'black', landcolor='tan', showlakes=False)\n",
    "\n",
    "# Set the text for each text box popping up when hovering over a point\n",
    "fig_places.update_traces(customdata = third_output[[\"placeAdress\", \"placeName\", \"numPeopleVisited\", \"numPeopleWant\", \"numPeopleWant\"]],\n",
    "                  hovertemplate='%{customdata[1]}<br>Address: %{customdata[0]}<br>Number of people that have visited the place: %{customdata[2]}<br>')\n",
    "\n",
    "fig_places.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
